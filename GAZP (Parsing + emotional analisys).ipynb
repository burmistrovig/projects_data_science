{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d59cf60",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36eb767a",
   "metadata": {},
   "outputs": [],
   "source": [
    "shares = ['ALRS','GAZP', 'SIBN','PLZL','GMKN','MAGN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "502d0004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sqlite3\n",
    "\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "from pytz import timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "049a5e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36',\n",
    "    'accept': '*/*',\n",
    "    'content-type': 'application/x-www-form-urlencoded',\n",
    "    'accept-encoding': 'gzip, deflate, br',\n",
    "    'accept-language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7'\n",
    "}\n",
    "link = 'https://www.tinkoff.ru/api/invest-gw/social/v1/post/instrument/{}?limit=50&appName=invest&platform=web&cursor={}'\n",
    "\n",
    "\n",
    "conn = sqlite3.connect('pulse_lesson.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "### –ü–µ—Ä–µ–¥–µ–ª–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ë–î —á—Ç–æ–±—ã –º–æ–∂–Ω–æ –±—ã–ª–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è !!!!!\n",
    "\n",
    "def _init_database():\n",
    "    with open('C:/Users/Iljam/create_table.sql', 'r') as f:\n",
    "        sql = f.read()\n",
    "    cursor.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "def insert(table_name: str, column_data: dict) -> None:\n",
    "    columns = ', '.join(column_data.keys())\n",
    "    values = [tuple(column_data.values())]\n",
    "    placeholders = ', '.join('?' * len(column_data.keys()))\n",
    "    cursor.executemany(\n",
    "        f'insert into {table_name}({columns}) values({placeholders})', values\n",
    "    )\n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "def get_instruments(lst: list):\n",
    "    data = []\n",
    "    for item in lst:\n",
    "        line = f\"{item.get('briefName')} !! {item.get('ticker')} !! {item.get('price')} !! {item.get('lastPrice')}\"\n",
    "        data.append(line)\n",
    "    return data\n",
    "\n",
    "\n",
    "def convert_inserted(line):\n",
    "    utc_time = datetime.strptime(line, '%Y-%m-%dT%H:%M:%S.%f%z')\\\n",
    "        .replace(tzinfo=timezone('utc')).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    return utc_time\n",
    "\n",
    "\n",
    "def get_cursor(url: str, ticker: str, cursor_number: str) -> str:\n",
    "    link = url.format(ticker, cursor_number)\n",
    "    session = requests.Session()\n",
    "    data = session.get(link, headers=headers, stream=True)\n",
    "    cursor_number = data.json().get('payload').get('nextCursor')\n",
    "    return cursor_number\n",
    "\n",
    "\n",
    "def get_data_from_api(url: str, ticker: str, cursor_number: str) -> None:\n",
    "    link = url.format(ticker, cursor_number)\n",
    "    session = requests.Session()\n",
    "    data = session.get(link, headers=headers, stream=True)\n",
    "    raw_data = data.json().get('payload').get('items')\n",
    "    \n",
    "\n",
    "    for i in raw_data:\n",
    "        my_data = {\n",
    "            'ticker': ticker,\n",
    "            'id': i.get('id')[:7],\n",
    "            'inserted': convert_inserted(i.get('inserted')),\n",
    "            'instruments': ', '.join(get_instruments(i[\"content\"].get('instruments'))),\n",
    "            'likesCount': i['reactions'].get('totalCount'),\n",
    "            'nickname': i.get('nickname'),\n",
    "            'commentsCount': i.get('commentsCount'),\n",
    "            'parse_date': datetime.today().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'text': i.get('content').get('text')\n",
    "        }\n",
    "        insert('dt_pulse_GAZP', my_data)\n",
    "\n",
    "\n",
    "def get_data_from_ticker(ticker: str):\n",
    "    max_cursor = '999999999999999'\n",
    "    for _ in tqdm(range(1, 100)):\n",
    "        get_data_from_api(link, ticker, max_cursor)\n",
    "        sleep(1.8)\n",
    "        max_cursor = get_cursor(link, ticker, max_cursor)\n",
    "        sleep(1)\n",
    "\n",
    "\n",
    "def main():\n",
    "    get_data_from_ticker(\"F\")\n",
    "    \n",
    "#if __name__ == '__main__':\n",
    "    #main()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae0b4a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlite3 import connect\n",
    "conn = connect('pulse_lesson.db')\n",
    "db_name = \"dt_pulse_GAZP\"\n",
    "df = pd.read_sql(sql= f\"SELECT * FROM {db_name}\", con=conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5d532f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['ticker'] == \"GAZP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "976a98bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_id</th>\n",
       "      <th>ticker</th>\n",
       "      <th>id</th>\n",
       "      <th>inserted</th>\n",
       "      <th>instruments</th>\n",
       "      <th>likesCount</th>\n",
       "      <th>nickname</th>\n",
       "      <th>commentsCount</th>\n",
       "      <th>parse_date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>GAZP</td>\n",
       "      <td>133dd47</td>\n",
       "      <td>2023-01-12 15:32:38</td>\n",
       "      <td>Yandex !! YNDX !! 1894.4 !! 1894.6, –°–±–µ—Ä –ë–∞–Ω–∫ ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Kadey_investor</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-12 15:34:25</td>\n",
       "      <td>–°–Ω–æ–≤–∞ –ø–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ —Å—á–µ—Ç–∞ üòä –≤ —ç—Ç–æ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>GAZP</td>\n",
       "      <td>f9e8fde</td>\n",
       "      <td>2023-01-12 15:24:37</td>\n",
       "      <td>–ì–∞–∑–ø—Ä–æ–º !! GAZP !! 165.03 !! 164.7</td>\n",
       "      <td>0</td>\n",
       "      <td>Rauf_</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-12 15:34:25</td>\n",
       "      <td>{$GAZP} –í—Å–µ–º –ø—Ä–∏–≤–µ—Ç ,–ø–æ–¥—Å–∫–∞–∂–∏—Ç–µ –∫—Ç–æ –∑–Ω–∞–µ—Ç –ø–æ –ò...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>GAZP</td>\n",
       "      <td>dbbed1e</td>\n",
       "      <td>2023-01-12 15:13:39</td>\n",
       "      <td>–ì–∞–∑–ø—Ä–æ–º !! GAZP !! 164.98 !! 164.7</td>\n",
       "      <td>2</td>\n",
       "      <td>IvanVer</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-12 15:34:25</td>\n",
       "      <td>{$GAZP}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>GAZP</td>\n",
       "      <td>3f5e13d</td>\n",
       "      <td>2023-01-12 14:57:08</td>\n",
       "      <td>–ì–∞–∑–ø—Ä–æ–º !! GAZP !! 165.02 !! 164.7</td>\n",
       "      <td>0</td>\n",
       "      <td>Slim78</td>\n",
       "      <td>7</td>\n",
       "      <td>2023-01-12 15:34:25</td>\n",
       "      <td>{$GAZP} –ö–æ–≥–¥–∞ –¥–∏–≤–∏–¥–µ–Ω–¥–æ–≤ –∂–¥–∞—Ç—å?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>GAZP</td>\n",
       "      <td>d56143e</td>\n",
       "      <td>2023-01-12 14:56:13</td>\n",
       "      <td>–ì–∞–∑–ø—Ä–æ–º !! GAZP !! 165.01 !! 164.7</td>\n",
       "      <td>5</td>\n",
       "      <td>INVESTOMATIKA</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-12 15:34:25</td>\n",
       "      <td>üìà –í –ø—Ä–µ–¥—ã–¥—É—â–∏–π —Ç–æ—Ä–≥–æ–≤—ã–π –¥–µ–Ω—å –∞–∫—Ü–∏–∏ –∫–æ–º–ø–∞–Ω–∏–∏ –ì–∞...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90930</th>\n",
       "      <td>None</td>\n",
       "      <td>GAZP</td>\n",
       "      <td>c0b9eb0</td>\n",
       "      <td>2023-01-20 13:22:19</td>\n",
       "      <td>–ì–∞–∑–ø—Ä–æ–º !! GAZP !! 158.96 !! 159.97</td>\n",
       "      <td>0</td>\n",
       "      <td>simivann</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-23 22:06:59</td>\n",
       "      <td>{$GAZP} –≤—Ä–æ–¥–µ –ø–∞–∫–∏—Å—Ç–∞–Ω—Å–∫–∏–π –ø–æ—Ç–æ–∫, –≤—ã—Ö–æ–¥ –Ω–∞ –±–µ–∑...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90931</th>\n",
       "      <td>None</td>\n",
       "      <td>GAZP</td>\n",
       "      <td>5965cf7</td>\n",
       "      <td>2023-01-20 13:20:44</td>\n",
       "      <td>–ì–∞–∑–ø—Ä–æ–º !! GAZP !! 159.05 !! 159.97</td>\n",
       "      <td>8</td>\n",
       "      <td>A0f</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-23 22:06:59</td>\n",
       "      <td>{$GAZP} –ø–æ—Ö–æ–∂–µ –±–∞–Ω–∫—Ä–æ—Ç, –æ–¥–Ω–∞ –Ω–∞–¥–µ–∂–¥–∞, —á—Ç–æ –µ–≥–æ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90932</th>\n",
       "      <td>None</td>\n",
       "      <td>GAZP</td>\n",
       "      <td>1fd2a11</td>\n",
       "      <td>2023-01-20 13:18:51</td>\n",
       "      <td>–ì–∞–∑–ø—Ä–æ–º !! GAZP !! 159.06 !! 159.97</td>\n",
       "      <td>0</td>\n",
       "      <td>chapokedy</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-23 22:06:59</td>\n",
       "      <td>{$GAZP} –ë–∞—é—à–∫–∏ –±–∞—é</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90933</th>\n",
       "      <td>None</td>\n",
       "      <td>GAZP</td>\n",
       "      <td>119ad6b</td>\n",
       "      <td>2023-01-20 13:18:50</td>\n",
       "      <td>–ì–∞–∑–ø—Ä–æ–º !! GAZP !! 159.06 !! 159.97</td>\n",
       "      <td>3</td>\n",
       "      <td>qlex</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-23 22:06:59</td>\n",
       "      <td>{$GAZP} –±–æ–∫–æ–≤–∏–∫ –∏ –Ω—ã–Ω–µ —Ç–∞–º, –≤—Å–µ–º —Ö–æ—Ä–æ—à–µ–≥–æ –Ω–∞—Å—Ç...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90934</th>\n",
       "      <td>None</td>\n",
       "      <td>GAZP</td>\n",
       "      <td>15cfaa9</td>\n",
       "      <td>2023-01-20 13:18:38</td>\n",
       "      <td>–ì–∞–∑–ø—Ä–æ–º !! GAZP !! 159.06 !! 159.97</td>\n",
       "      <td>1</td>\n",
       "      <td>Vovandemord</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-23 22:06:59</td>\n",
       "      <td>{$GAZP} –ø–æ–≤–µ–∑–ª–æ —Ç–æ–º—É –∫—Ç–æ –≤ —à–æ—Ä—Ç –≤—Å—Ç–∞–ª</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90935 rows √ó 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      src_id ticker       id             inserted  \\\n",
       "0       None   GAZP  133dd47  2023-01-12 15:32:38   \n",
       "1       None   GAZP  f9e8fde  2023-01-12 15:24:37   \n",
       "2       None   GAZP  dbbed1e  2023-01-12 15:13:39   \n",
       "3       None   GAZP  3f5e13d  2023-01-12 14:57:08   \n",
       "4       None   GAZP  d56143e  2023-01-12 14:56:13   \n",
       "...      ...    ...      ...                  ...   \n",
       "90930   None   GAZP  c0b9eb0  2023-01-20 13:22:19   \n",
       "90931   None   GAZP  5965cf7  2023-01-20 13:20:44   \n",
       "90932   None   GAZP  1fd2a11  2023-01-20 13:18:51   \n",
       "90933   None   GAZP  119ad6b  2023-01-20 13:18:50   \n",
       "90934   None   GAZP  15cfaa9  2023-01-20 13:18:38   \n",
       "\n",
       "                                             instruments  likesCount  \\\n",
       "0      Yandex !! YNDX !! 1894.4 !! 1894.6, –°–±–µ—Ä –ë–∞–Ω–∫ ...           0   \n",
       "1                     –ì–∞–∑–ø—Ä–æ–º !! GAZP !! 165.03 !! 164.7           0   \n",
       "2                     –ì–∞–∑–ø—Ä–æ–º !! GAZP !! 164.98 !! 164.7           2   \n",
       "3                     –ì–∞–∑–ø—Ä–æ–º !! GAZP !! 165.02 !! 164.7           0   \n",
       "4                     –ì–∞–∑–ø—Ä–æ–º !! GAZP !! 165.01 !! 164.7           5   \n",
       "...                                                  ...         ...   \n",
       "90930                –ì–∞–∑–ø—Ä–æ–º !! GAZP !! 158.96 !! 159.97           0   \n",
       "90931                –ì–∞–∑–ø—Ä–æ–º !! GAZP !! 159.05 !! 159.97           8   \n",
       "90932                –ì–∞–∑–ø—Ä–æ–º !! GAZP !! 159.06 !! 159.97           0   \n",
       "90933                –ì–∞–∑–ø—Ä–æ–º !! GAZP !! 159.06 !! 159.97           3   \n",
       "90934                –ì–∞–∑–ø—Ä–æ–º !! GAZP !! 159.06 !! 159.97           1   \n",
       "\n",
       "             nickname  commentsCount           parse_date  \\\n",
       "0      Kadey_investor              0  2023-01-12 15:34:25   \n",
       "1               Rauf_              1  2023-01-12 15:34:25   \n",
       "2             IvanVer              1  2023-01-12 15:34:25   \n",
       "3              Slim78              7  2023-01-12 15:34:25   \n",
       "4       INVESTOMATIKA              3  2023-01-12 15:34:25   \n",
       "...               ...            ...                  ...   \n",
       "90930        simivann              3  2023-01-23 22:06:59   \n",
       "90931             A0f              2  2023-01-23 22:06:59   \n",
       "90932       chapokedy              0  2023-01-23 22:06:59   \n",
       "90933            qlex              0  2023-01-23 22:06:59   \n",
       "90934     Vovandemord              0  2023-01-23 22:06:59   \n",
       "\n",
       "                                                    text  \n",
       "0      –°–Ω–æ–≤–∞ –ø–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ —Å—á–µ—Ç–∞ üòä –≤ —ç—Ç–æ...  \n",
       "1      {$GAZP} –í—Å–µ–º –ø—Ä–∏–≤–µ—Ç ,–ø–æ–¥—Å–∫–∞–∂–∏—Ç–µ –∫—Ç–æ –∑–Ω–∞–µ—Ç –ø–æ –ò...  \n",
       "2                                                {$GAZP}  \n",
       "3                        {$GAZP} –ö–æ–≥–¥–∞ –¥–∏–≤–∏–¥–µ–Ω–¥–æ–≤ –∂–¥–∞—Ç—å?  \n",
       "4      üìà –í –ø—Ä–µ–¥—ã–¥—É—â–∏–π —Ç–æ—Ä–≥–æ–≤—ã–π –¥–µ–Ω—å –∞–∫—Ü–∏–∏ –∫–æ–º–ø–∞–Ω–∏–∏ –ì–∞...  \n",
       "...                                                  ...  \n",
       "90930  {$GAZP} –≤—Ä–æ–¥–µ –ø–∞–∫–∏—Å—Ç–∞–Ω—Å–∫–∏–π –ø–æ—Ç–æ–∫, –≤—ã—Ö–æ–¥ –Ω–∞ –±–µ–∑...  \n",
       "90931  {$GAZP} –ø–æ—Ö–æ–∂–µ –±–∞–Ω–∫—Ä–æ—Ç, –æ–¥–Ω–∞ –Ω–∞–¥–µ–∂–¥–∞, —á—Ç–æ –µ–≥–æ ...  \n",
       "90932                                 {$GAZP} –ë–∞—é—à–∫–∏ –±–∞—é  \n",
       "90933  {$GAZP} –±–æ–∫–æ–≤–∏–∫ –∏ –Ω—ã–Ω–µ —Ç–∞–º, –≤—Å–µ–º —Ö–æ—Ä–æ—à–µ–≥–æ –Ω–∞—Å—Ç...  \n",
       "90934              {$GAZP} –ø–æ–≤–µ–∑–ª–æ —Ç–æ–º—É –∫—Ç–æ –≤ —à–æ—Ä—Ç –≤—Å—Ç–∞–ª  \n",
       "\n",
       "[90935 rows x 10 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92773dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotional analisis\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('blanchefort/rubert-base-cased-sentiment-rurewiews')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('blanchefort/rubert-base-cased-sentiment-rurewiews', return_dict=True)\n",
    "\n",
    "\n",
    "def predict(text):\n",
    "    inputs = tokenizer(text, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "    outputs = model(**inputs)\n",
    "    predicted = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "    predicted = torch.argmax(predicted, dim=1).numpy()\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b41a8b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a49ea5b3e46c4ef88c8b3f40c3e60f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/935 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-a7646844130d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtonality_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m90000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtonality_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-38-7832860d374f>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1528\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1530\u001b[1;33m         outputs = self.bert(\n\u001b[0m\u001b[0;32m   1531\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1532\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    994\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m         )\n\u001b[1;32m--> 996\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m    997\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    581\u001b[0m                 )\n\u001b[0;32m    582\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m                 layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m    584\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    468\u001b[0m         \u001b[1;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 470\u001b[1;33m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[0;32m    471\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    398\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     ):\n\u001b[1;32m--> 400\u001b[1;33m         self_outputs = self.self(\n\u001b[0m\u001b[0;32m    401\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m         \u001b[1;31m# Normalize the attention scores to probabilities.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m         \u001b[0mattention_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# This is actually dropping out entire tokens to attend to, which might\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m   1259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1260\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1261\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1263\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1816\u001b[0m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"softmax\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1817\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1818\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1819\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1820\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tonality_list = []\n",
    "for i in tqdm(range(len(df)-90000)):\n",
    "    tonality_list.append(predict(df.loc[i, 'text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0669b64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tonality_type'] = tonality_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af8ec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_sql(\"df_GAZP\", con = conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb92ba5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['inserted'] = pd.to_datetime(df['inserted'])\n",
    "date_list = []\n",
    "for date in tqdm(range(len(df))):\n",
    "    date_list.append(df.loc[date, 'inserted'].date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2ab610",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5931a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_list = []\n",
    "day_list = []\n",
    "for element in df['date']:\n",
    "    day_list.append(element.day)\n",
    "    month_list.append(element.month)\n",
    "    \n",
    "df['month'] = month_list\n",
    "df['day'] = day_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51a9aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_list = []\n",
    "for element in df['date']:\n",
    "    year_list.append(element.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0479a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_checkpoint = 'cointegrated/rubert-tiny-sentiment-balanced'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    \n",
    "def get_sentiment(text, return_type='label'):\n",
    "    \"\"\" Calculate sentiment of a text. `return_type` can be 'label', 'score' or 'proba' \"\"\"\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True).to(model.device)\n",
    "        proba = torch.sigmoid(model(**inputs).logits).cpu().numpy()[0]\n",
    "    if return_type == 'label':\n",
    "        return model.config.id2label[proba.argmax()]\n",
    "    elif return_type == 'score':\n",
    "        return proba.dot([-1, 0, 1])\n",
    "    return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc409a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = []\n",
    "score_list = []\n",
    "\n",
    "for i in tqdm(range(len(df))):\n",
    "    label_list.append(get_sentiment(df.loc[i, 'text'], 'label'))\n",
    "    score_list.append(get_sentiment(df.loc[i, 'text'], 'score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e383e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tonality_label'] = label_list\n",
    "df['tonality_score'] = score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00362b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_checkpoint = 'cointegrated/rubert-tiny-toxicity'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    \n",
    "def text2toxicity(text, aggregate=True):\n",
    "    \"\"\" Calculate toxicity of a text (if aggregate=True) or a vector of toxicity aspects (if aggregate=False)\"\"\"\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True).to(model.device)\n",
    "        proba = torch.sigmoid(model(**inputs).logits).cpu().numpy()\n",
    "    if isinstance(text, str):\n",
    "        proba = proba[0]\n",
    "    if aggregate:\n",
    "        return 1 - proba.T[0] * (1 - proba.T[-1])\n",
    "    return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9ff920",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_list = []\n",
    "\n",
    "for i in tqdm(range(len(df))):\n",
    "    toxicity_list.append(text2toxicity(df.loc[i, 'text'], True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763fd89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['toxic_value'] = toxicity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8843ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['src_id','id','instruments','nickname','parse_date','day'], axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2c6e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_sql('df_GAZP_2', con = conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436e3cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification, AutoTokenizer\n",
    "\n",
    "LABELS = ['neutral', 'happiness', 'sadness', 'enthusiasm', 'fear', 'anger', 'disgust']\n",
    "tokenizer = AutoTokenizer.from_pretrained('Aniemore/rubert-tiny2-russian-emotion-detection')\n",
    "model = BertForSequenceClassification.from_pretrained('Aniemore/rubert-tiny2-russian-emotion-detection')\n",
    "\n",
    "\n",
    "def predict_emotion(text: str) -> str:\n",
    "    \"\"\"\n",
    "        We take the input text, tokenize it, pass it through the model, and then return the predicted label\n",
    "        :param text: The text to be classified\n",
    "        :type text: str\n",
    "        :return: The predicted emotion\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "    outputs = model(**inputs)\n",
    "    predicted = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "    predicted = torch.argmax(predicted, dim=1).numpy()\n",
    "        \n",
    "    return LABELS[predicted[0]]\n",
    "\n",
    "\n",
    "def predict_emotions(text: str) -> list:\n",
    "    \"\"\"\n",
    "        It takes a string of text, tokenizes it, feeds it to the model, and returns a dictionary of emotions and their\n",
    "        probabilities\n",
    "        :param text: The text you want to classify\n",
    "        :type text: str\n",
    "        :return: A dictionary of emotions and their probabilities.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "    outputs = model(**inputs)\n",
    "    predicted = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "    emotions_list = {}\n",
    "    for i in range(len(predicted.numpy()[0].tolist())):\n",
    "        emotions_list[LABELS[i]] = predicted.numpy()[0].tolist()[i]\n",
    "    return emotions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e17626",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_list = []\n",
    "\n",
    "for i in tqdm(range(len(df))):\n",
    "    emotion_list.append(predict_emotion(df.loc[i, 'text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae167217",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['emotions'] = emotion_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcf544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e6a4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_year = []\n",
    "for date in df['date']:\n",
    "    month_year.append(date.strftime(\"%Y-%m\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c06851",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['month_year'] = month_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623a65fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df = df.pivot_table(index = 'month_year', values = 'tonality_score', aggfunc='mean')\n",
    "pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34f5720",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (16,8))\n",
    "sns.barplot(data = pivot_df, x=pivot_df.index, y= 'tonality_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4280691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "another_agg = df['tonality_label'].value_counts()\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "plt.pie(x = another_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027afddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_sql(\"df_pulse_GAZP_6\", con = conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e114fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79e5c39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5952d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f89035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e95bcd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a42eee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e9e831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd724a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5519611",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b208c068",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa0e306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
